{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('/Users/lmps/github/ba-text-mining/lab_sessions/lab4/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "   a_dict = {\n",
    "      'words': token, 'pos': pos\n",
    "   }\n",
    "   training_features.append(a_dict)\n",
    "   training_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader('/Users/lmps/github/ba-text-mining/lab_sessions/lab4/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "        'words': token, 'pos': pos\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many instances are in train and test?\n",
      "\n",
      "Number of instances in the training data is: 203621\tPercentage is: 81.43015964423968\n",
      "Number of instances in the test data is: 46435\tPercentage is: 18.56984035576031\n",
      "\n",
      "Provide a frequency distribution of the NERC labels\n",
      "\n",
      "Test labels are: dict_items([('O', 38323), ('B-LOC', 1668), ('B-PER', 1617), ('I-PER', 1156), ('I-LOC', 257), ('B-MISC', 702), ('I-MISC', 216), ('B-ORG', 1661), ('I-ORG', 835)])\n",
      "Train labels are: dict_items([('B-ORG', 6321), ('O', 169578), ('B-MISC', 3438), ('B-PER', 6600), ('I-PER', 4528), ('B-LOC', 7140), ('I-ORG', 3704), ('I-MISC', 1155), ('I-LOC', 1157)])\n",
      "\n",
      "\n",
      "Feature: B-ORG\n",
      "Training Count is: 6321\t\tPercentage is: 3.1042967080998523\n",
      "Test Count is: 1661\t\tPercentage is: 3.5770431786368038\n",
      "\n",
      "Feature: O\n",
      "Training Count is: 169578\t\tPercentage is: 83.2811939829389\n",
      "Test Count is: 38323\t\tPercentage is: 82.53041886508022\n",
      "\n",
      "Feature: B-MISC\n",
      "Training Count is: 3438\t\tPercentage is: 1.6884309575142051\n",
      "Test Count is: 702\t\tPercentage is: 1.5117906751372887\n",
      "\n",
      "Feature: B-PER\n",
      "Training Count is: 6600\t\tPercentage is: 3.24131597428556\n",
      "Test Count is: 1617\t\tPercentage is: 3.482287067944438\n",
      "\n",
      "Feature: I-PER\n",
      "Training Count is: 4528\t\tPercentage is: 2.223739201752275\n",
      "Test Count is: 1156\t\tPercentage is: 2.48950145364488\n",
      "\n",
      "Feature: B-LOC\n",
      "Training Count is: 7140\t\tPercentage is: 3.506514553999833\n",
      "Test Count is: 1668\t\tPercentage is: 3.592118014428771\n",
      "\n",
      "Feature: I-ORG\n",
      "Training Count is: 3704\t\tPercentage is: 1.8190658134475326\n",
      "Test Count is: 835\t\tPercentage is: 1.7982125551846666\n",
      "\n",
      "Feature: I-MISC\n",
      "Training Count is: 1155\t\tPercentage is: 0.5672302954999731\n",
      "Test Count is: 216\t\tPercentage is: 0.4651663615807042\n",
      "\n",
      "Feature: I-LOC\n",
      "Training Count is: 1157\t\tPercentage is: 0.5682125124618777\n",
      "Test Count is: 257\t\tPercentage is: 0.5534618283622268\n",
      "\n",
      "The instances for each label is quite similar for both training and test datasets. For both the test and training dataset, about 13% of the total instances are labeled LOC, PER, ORG and MISC, where it is almost equally distributed (~3%) for each label\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import pandas as pd\n",
    "\n",
    "# How many instances are in train and test?\n",
    "test_len = len(test.iob_words())\n",
    "train_len = len(train.iob_words())\n",
    "print(\"How many instances are in train and test?\\n\")\n",
    "print(\"Number of instances in the training data is: {}\\tPercentage is: {}\\nNumber of instances in the test data is: {}\\tPercentage is: {}\\n\".format(\n",
    "    train_len, train_len/(test_len + train_len) * 100, test_len, test_len/(test_len + train_len) * 100))\n",
    "\n",
    "# Provide a frequency distribution of the NERC labels\n",
    "print(\"Provide a frequency distribution of the NERC labels\\n\")\n",
    "train_labels = Counter(training_gold_labels)\n",
    "test_labels = Counter(test_gold_labels)\n",
    "\n",
    "print(\"Test labels are: {}\\nTrain labels are: {}\\n\".format(test_labels.items(), train_labels.items()))\n",
    "\n",
    "for key in train_labels.keys():\n",
    "    print('\\nFeature: {}'.format(key))\n",
    "    print('Training Count is: {}\\t\\tPercentage is: {}'.format(train_labels[key],  train_labels[key]/train_len * 100))\n",
    "    print('Test Count is: {}\\t\\tPercentage is: {}'.format(test_labels[key], test_labels[key]/test_len * 100))\n",
    "\n",
    "print(\"\\nThe instances for each label is quite similar for both training and test datasets. For both the test and training dataset, about 13% of the total instances are labeled LOC, PER, ORG and MISC, where it is almost equally distributed (~3%) for each label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "concatenation = training_features + test_features\n",
    "the_array = vec.fit_transform(concatenation)\n",
    "\n",
    "train_array = the_array[:len(training_features)]\n",
    "test_array = the_array[len(training_features):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report)**. The train (lin_clf.fit) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lin_clf.fit(train_array, training_gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.78      0.81      0.79      1592\n",
      "      B-MISC       0.66      0.78      0.72       596\n",
      "       B-ORG       0.52      0.79      0.63      1088\n",
      "       B-PER       0.44      0.86      0.58       821\n",
      "       I-LOC       0.53      0.62      0.57       220\n",
      "      I-MISC       0.59      0.57      0.58       223\n",
      "       I-ORG       0.47      0.70      0.56       555\n",
      "       I-PER       0.87      0.33      0.48      3028\n",
      "           O       0.98      0.98      0.98     38312\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.65      0.72      0.65     46435\n",
      "weighted avg       0.93      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = model.predict(test_array)\n",
    "print(classification_report(predicted, test_gold_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer</b>\n",
    "\n",
    "The O label has the highest performance with a f1-score of 0.98. This can be attributed to the number of instances present (83% of the instances) in the training dataset, since (usually) the greater the representation of a label in the training data, the better the performance. Similarly, B-LOC and B-MISC are the better performing labels, with an f1-score > 0.7. This is surprising as B-MISC has a low representation in the training data (~2% of the instances), however, it achieved a relatively high f1-score. This could be due to the high recall.\n",
    "The NERC label with the lowest f1-score is I-PER even though it has a high number of occurences in the data. However, I-PER does have a high precision score but what impacts it's f1-score is the low recall score it achieved. Similary, B-PER has a high recall rate but a low precision score which brings down it's f1-score. For other NERC labels with low f1-scores (I-ORG, I-MISC, I-LOC), the classifier performs poorly on both precision and recall. This can be attributed to the their low number of occurences in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(\"/Users/lmps/github/ba-text-mining/lab_sessions/lab2/GoogleNews-vectors-negative300.bin.gz\", binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_conversion(data):\n",
    "    embedded_data = []\n",
    "    for word in data:\n",
    "        # we check if our word\n",
    "        # is inside the model vocabulary (loaded with the Google word2vec embeddings)\n",
    "        if word in word_embedding_model:\n",
    "            # in this case the word was found and vector is assigned with its embedding vector as the value\n",
    "            vector=word_embedding_model[word]\n",
    "        else:\n",
    "            # if the word does not exist in the embeddings vocabulary,\n",
    "            # we create a vector with all zeros.\n",
    "            # The Google word2vec model has 300 dimensions so we creat a vector with 300 zeros\n",
    "            vector=[0]*300\n",
    "        embedded_data.append(vector)\n",
    "    return embedded_data\n",
    "\n",
    "training_embeddings = embeddings_conversion([token for token, pos, ne_label in train.iob_words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lin_cf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lin_cf.fit(training_embeddings,training_gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.80      0.76      0.78      1760\n",
      "      B-MISC       0.70      0.72      0.71       674\n",
      "       B-ORG       0.64      0.69      0.66      1537\n",
      "       B-PER       0.67      0.75      0.71      1449\n",
      "       I-LOC       0.42      0.51      0.46       212\n",
      "      I-MISC       0.54      0.60      0.57       192\n",
      "       I-ORG       0.33      0.48      0.39       577\n",
      "       I-PER       0.50      0.59      0.54       988\n",
      "           O       0.99      0.97      0.98     39046\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.62      0.68      0.64     46435\n",
      "weighted avg       0.93      0.93      0.93     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = embeddings_conversion([token for token, pos,ne_label in test.iob_words()])\n",
    "predicted = new_lin_cf.predict(test_embeddings)\n",
    "print(classification_report(predicted, test_gold_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Answer </b>\n",
    "When we compare the classification report of both models, we see some interesting differences. The model that uses the features as an input has a more evenly balanced performance (f1-score) across all the various class labels (low: 0.48, high: 0.98), whereas the model that uses the embeddings has a less balanced performance (low: 0.39, high: 0.98)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = '/Users/lmps/github/ba-text-mining/lab_sessions/lab4/ner_dataset.csv'\n",
    "kaggle_dataset = pd.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "\n",
    "df_train_labels = df_train['Tag']\n",
    "df_train_features = []\n",
    "df_test_labels = df_test['Tag']\n",
    "df_test_features = []\n",
    "\n",
    "for index, entry in df_test.iterrows():\n",
    "    df_test_features.append({'word': entry.Word, 'pos': entry.POS})\n",
    "\n",
    "for index, entry in df_train.iterrows():\n",
    "    df_train_features.append({'word': entry.Word, 'pos': entry.POS})\n",
    "\n",
    "print(len(df_train_features), len(df_test_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "kaggle_concatenation = df_train_features + df_test_features\n",
    "kaggle_array = vec.fit_transform(kaggle_concatenation)\n",
    "\n",
    "kaggle_train = kaggle_array[:len(df_train_features)]\n",
    "kaggle_test = kaggle_array[len(df_train_features):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_lin_clf = svm.LinearSVC()\n",
    "kaggle_model = kaggle_lin_clf.fit(kaggle_train, df_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "* Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         0\n",
      "       B-eve       0.00      0.00      0.00         2\n",
      "       B-geo       0.76      0.80      0.78       697\n",
      "       B-gpe       0.92      0.96      0.94       283\n",
      "       B-nat       0.50      1.00      0.67         4\n",
      "       B-org       0.51      0.64      0.57       319\n",
      "       B-per       0.53      0.81      0.64       220\n",
      "       B-tim       0.76      0.91      0.83       331\n",
      "       I-art       0.00      0.00      0.00         4\n",
      "       I-eve       0.00      0.00      0.00         3\n",
      "       I-geo       0.50      0.74      0.60       105\n",
      "       I-gpe       0.50      1.00      0.67         1\n",
      "       I-nat       1.00      0.80      0.89         5\n",
      "       I-org       0.44      0.65      0.53       217\n",
      "       I-per       0.90      0.42      0.57       692\n",
      "       I-tim       0.08      0.41      0.14        22\n",
      "           O       0.99      0.98      0.99     17095\n",
      "\n",
      "    accuracy                           0.94     20000\n",
      "   macro avg       0.49      0.60      0.52     20000\n",
      "weighted avg       0.95      0.94      0.94     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "kaggle_predicted = kaggle_model.predict(kaggle_test)\n",
    "print(classification_report(kaggle_predicted, df_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Answer </b>\n",
    "\n",
    "NERC labels B-art, B-eve, B-nat, I-art, I-eve have extremely low f-1 scores of 0. This is most likely because these features were not represented enough in the train data set, moreover, there were only few instances to test them in the test set, thus decreasing the accuracy of the quantative analysis. \n",
    "\n",
    "NERC labels O, B-gpe, B-tim, and I-nat have the highest f-1 scores. The extremely high score (0.99) of the O label can be attributed to it's very high occurence in the dataset (17095/20000 instances). \n",
    "\n",
    "Overall, the model has a very high accuracy and weighted average of 0.94. However, it has a low macro average. One explanation for the low macro average score could be that there are many labels that resulted in a f1-score of 0. This thus pulls down the macro average score of the model. Hence, macro average is not a good metric since the performance across classes is not balanced. However, because the instances of such labels are very little (e.g. B-art  has 0 and B-eve has 2 support) their f1-score will be penalised in the weighted average score of 0.94. Similarly, the high weighted average score of 0.94 could be explained by the O label, where it has the most instances and 0.99 f1-score. Hence, neither macro nor weighted average can provide a good indication of the performance of our model due to the class and performance imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
